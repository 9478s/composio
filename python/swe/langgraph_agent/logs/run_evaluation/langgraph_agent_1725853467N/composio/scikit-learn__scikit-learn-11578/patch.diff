diff --git a/doc/modules/linear_model.rst b/doc/modules/linear_model.rst
index bb4a9e4..7f594ef 100644
--- a/doc/modules/linear_model.rst
+++ b/doc/modules/linear_model.rst
@@ -823,6 +823,12 @@ with 'log' loss.
    :ref:`l1_feature_selection`.
 
 :class:`LogisticRegressionCV` implements Logistic Regression with
+built-in cross-validation to find the optimal C and l1_ratio parameters.
+The 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers are found to be faster
+for high-dimensional dense data, due to warm-starting. For the
+multiclass case, if `multi_class` is set to 'ovr', an optimal C is selected
+for each class and if `multi_class` is set to 'multinomial', an optimal C is
+selected by minimizing the cross-entropy loss.
 builtin cross-validation to find out the optimal C parameter.
 "newton-cg", "sag", "saga" and "lbfgs" solvers are found to be faster
 for high-dimensional dense data, due to warm-starting. For the
diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index e4ea696..cca441b 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -922,7 +922,8 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         check_input=False, max_squared_sum=max_squared_sum,
         sample_weight=sample_weight)
 
-    log_reg = LogisticRegression(fit_intercept=fit_intercept)
+    log_reg = LogisticRegression(fit_intercept=fit_intercept, multi_class=multi_class,
+                              solver=solver, penalty=penalty, random_state=random_state)
 
     # The score method of Logistic Regression has a classes_ attribute.
     if multi_class == 'ovr':
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index 56be87f..033ffcc 100644
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -1260,3 +1260,26 @@ def test_warm_start_converge_LR():
         lr_ws.fit(X, y)
     lr_ws_loss = log_loss(y, lr_ws.predict_proba(X))
     assert_allclose(lr_no_ws_loss, lr_ws_loss, rtol=1e-5)
+
+def test_logistic_regression_cv_multi_class():
+    X, y = make_classification(n_samples=100, n_classes=3, n_informative=3, random_state=42)
+    clf = LogisticRegressionCV(multi_class='multinomial', cv=3, random_state=42)
+    clf.fit(X, y)
+    
+    # Check that the probabilities sum to 1
+    assert_almost_equal(clf.predict_proba(X).sum(axis=1), np.ones(X.shape[0]))
+    
+    # Check that the best_score_ is consistent with manual cross-validation
+    cv = StratifiedKFold(n_splits=3, random_state=42)
+    manual_scores = []
+    for train, test in cv.split(X, y):
+        lr = LogisticRegression(multi_class='multinomial', C=clf.C_[0], random_state=42)
+        lr.fit(X[train], y[train])
+        manual_scores.append(lr.score(X[test], y[test]))
+    assert_almost_equal(clf.best_score_, np.mean(manual_scores), decimal=5)
+
+    # Check that multi_class parameter is passed correctly
+    assert_equal(clf.estimator.multi_class, 'multinomial')
+
+    # Check that the coefficients have the correct shape for multinomial
+    assert_equal(clf.coef_.shape, (3, X.shape[1]))  # 3 classes, X.shape[1] features
