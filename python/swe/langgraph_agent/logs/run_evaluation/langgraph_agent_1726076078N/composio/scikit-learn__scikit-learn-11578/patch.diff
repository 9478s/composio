diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index e4ea696..f748c28 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -772,12 +772,12 @@ def logistic_regression_path(X, y, pos_class=None, Cs=10, fit_intercept=True,
 
 # helper function for LogisticCV
 def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
-                          scoring=None, fit_intercept=False,
-                          max_iter=100, tol=1e-4, class_weight=None,
-                          verbose=0, solver='lbfgs', penalty='l2',
-                          dual=False, intercept_scaling=1.,
-                          multi_class='ovr', random_state=None,
-                          max_squared_sum=None, sample_weight=None):
+                           scoring=None, fit_intercept=False,
+                           max_iter=100, tol=1e-4, class_weight=None,
+                           verbose=0, solver='lbfgs', penalty='l2',
+                           dual=False, intercept_scaling=1.,
+                           multi_class='ovr', random_state=None,
+                           max_squared_sum=None, sample_weight=None):
     """Computes scores across logistic_regression_path
 
     Parameters
@@ -899,6 +899,16 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
 
     n_iter : array, shape(n_cs,)
         Actual number of iteration for each Cs.
+
+    Notes
+    -----
+    You might get slightly different results with the solver liblinear than
+    with the others since this uses LIBLINEAR which penalizes the intercept.
+
+    The scores are calculated using the scoring parameter on the
+    LogisticRegression instance created for each C value. This instance
+    is configured with all parameters passed to this function, and the
+    C value is set for each iteration in the scoring loop.
     """
     _check_solver_option(solver, multi_class, penalty, dual)
 
@@ -921,8 +931,24 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         intercept_scaling=intercept_scaling, random_state=random_state,
         check_input=False, max_squared_sum=max_squared_sum,
         sample_weight=sample_weight)
-
-    log_reg = LogisticRegression(fit_intercept=fit_intercept)
+    # Create a LogisticRegression instance with the same parameters
+    # as those used in logistic_regression_path. This ensures consistency
+    # between the fitting process and the scoring process, especially
+    # for parameters like multi_class and solver.
+    log_reg = LogisticRegression(
+        fit_intercept=fit_intercept,
+        multi_class=multi_class,
+        solver=solver,
+        penalty=penalty,
+        dual=dual,
+        tol=tol,
+        intercept_scaling=intercept_scaling,
+        class_weight=class_weight,
+        random_state=random_state,
+        max_iter=max_iter
+    )
+    # Note: The 'C' parameter is not set here as it varies across the path.
+    # It will be set for each iteration in the scoring loop below.
 
     # The score method of Logistic Regression has a classes_ attribute.
     if multi_class == 'ovr':
@@ -930,8 +956,8 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
     elif multi_class == 'multinomial':
         log_reg.classes_ = np.unique(y_train)
     else:
-        raise ValueError("multi_class should be either multinomial or ovr, "
-                         "got %d" % multi_class)
+        raise ValueError("multi_class should be either 'ovr' or 'multinomial', "
+                         "got %s" % multi_class)
 
     if pos_class is not None:
         mask = (y_test == pos_class)
@@ -942,7 +968,11 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
 
     if isinstance(scoring, six.string_types):
         scoring = get_scorer(scoring)
-    for w in coefs:
+    for w, C in zip(coefs, Cs):
+        # Set the 'C' parameter for this iteration
+        # C is the inverse of regularization strength; must be a positive float.
+        # Smaller values specify stronger regularization.
+        log_reg.C = C
         if multi_class == 'ovr':
             w = w[np.newaxis, :]
         if fit_intercept:
@@ -952,6 +982,8 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
             log_reg.coef_ = w
             log_reg.intercept_ = 0.
 
+        # Set the 'C' parameter for this iteration
+        log_reg.C = C
         if scoring is None:
             scores.append(log_reg.score(X_test, y_test))
         else:
