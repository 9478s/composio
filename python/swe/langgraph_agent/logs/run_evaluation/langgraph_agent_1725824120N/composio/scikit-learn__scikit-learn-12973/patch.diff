diff --git a/sklearn/linear_model/least_angle.py b/sklearn/linear_model/least_angle.py
index 03b34a0..9e9ba51 100644
--- a/sklearn/linear_model/least_angle.py
+++ b/sklearn/linear_model/least_angle.py
@@ -1479,68 +1479,72 @@ class LassoLarsIC(LassoLars):
         self.eps = eps
         self.fit_path = True
 
-    def fit(self, X, y, copy_X=True):
-        """Fit the model using X, y as training data.
-
-        Parameters
-        ----------
-        X : array-like, shape (n_samples, n_features)
-            training data.
+def fit(self, X, y, copy_X=None):
+    """Fit the model using X, y as training data.
 
-        y : array-like, shape (n_samples,)
-            target values. Will be cast to X's dtype if necessary
+    Parameters
+    ----------
+    X : array-like, shape (n_samples, n_features)
+        training data.
 
-        copy_X : boolean, optional, default True
-            If ``True``, X will be copied; else, it may be overwritten.
+    y : array-like, shape (n_samples,)
+        target values. Will be cast to X's dtype if necessary
 
-        Returns
-        -------
-        self : object
-            returns an instance of self.
-        """
-        X, y = check_X_y(X, y, y_numeric=True)
+    copy_X : boolean, optional (default=None)
+        If None, use the `copy_X` value passed in the constructor.
+        If True, X will be copied; else, it may be overwritten.
 
-        X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(
-            X, y, self.fit_intercept, self.normalize, self.copy_X)
-        max_iter = self.max_iter
+    Returns
+    -------
+    self : object
+        returns an instance of self.
+    """
+    if copy_X is None:
+        copy_X = self.copy_X
 
-        Gram = self.precompute
+    X, y = check_X_y(X, y, y_numeric=True)
 
-        alphas_, active_, coef_path_, self.n_iter_ = lars_path(
-            X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,
-            method='lasso', verbose=self.verbose, max_iter=max_iter,
-            eps=self.eps, return_n_iter=True, positive=self.positive)
+    X, y, Xmean, ymean, Xstd = LinearModel._preprocess_data(
+        X, y, self.fit_intercept, self.normalize, copy_X)
+    max_iter = self.max_iter
 
-        n_samples = X.shape[0]
+    Gram = self.precompute
 
-        if self.criterion == 'aic':
-            K = 2  # AIC
-        elif self.criterion == 'bic':
-            K = log(n_samples)  # BIC
-        else:
-            raise ValueError('criterion should be either bic or aic')
+    alphas_, active_, coef_path_, self.n_iter_ = lars_path(
+        X, y, Gram=Gram, copy_X=copy_X, copy_Gram=True, alpha_min=0.0,
+        method='lasso', verbose=self.verbose, max_iter=max_iter,
+        eps=self.eps, return_n_iter=True, positive=self.positive)
 
-        R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals
-        mean_squared_error = np.mean(R ** 2, axis=0)
-        sigma2 = np.var(y)
+    n_samples = X.shape[0]
 
-        df = np.zeros(coef_path_.shape[1], dtype=np.int)  # Degrees of freedom
-        for k, coef in enumerate(coef_path_.T):
-            mask = np.abs(coef) > np.finfo(coef.dtype).eps
-            if not np.any(mask):
-                continue
-            # get the number of degrees of freedom equal to:
-            # Xc = X[:, mask]
-            # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs
-            df[k] = np.sum(mask)
-
-        self.alphas_ = alphas_
-        eps64 = np.finfo('float64').eps
-        self.criterion_ = (n_samples * mean_squared_error / (sigma2 + eps64) +
-                           K * df)  # Eqns. 2.15--16 in (Zou et al, 2007)
-        n_best = np.argmin(self.criterion_)
-
-        self.alpha_ = alphas_[n_best]
-        self.coef_ = coef_path_[:, n_best]
-        self._set_intercept(Xmean, ymean, Xstd)
-        return self
+    if self.criterion == 'aic':
+        K = 2  # AIC
+    elif self.criterion == 'bic':
+        K = log(n_samples)  # BIC
+    else:
+        raise ValueError('criterion should be either bic or aic')
+
+    R = y[:, np.newaxis] - np.dot(X, coef_path_)  # residuals
+    mean_squared_error = np.mean(R ** 2, axis=0)
+    sigma2 = np.var(y)
+
+    df = np.zeros(coef_path_.shape[1], dtype=np.int)  # Degrees of freedom
+    for k, coef in enumerate(coef_path_.T):
+        mask = np.abs(coef) > np.finfo(coef.dtype).eps
+        if not np.any(mask):
+            continue
+        # get the number of degrees of freedom equal to:
+        # Xc = X[:, mask]
+        # Trace(Xc * inv(Xc.T, Xc) * Xc.T) ie the number of non-zero coefs
+        df[k] = np.sum(mask)
+
+    self.alphas_ = alphas_
+    eps64 = np.finfo('float64').eps
+    self.criterion_ = (n_samples * mean_squared_error / (sigma2 + eps64) +
+                       K * df)  # Eqns. 2.15--16 in (Zou et al, 2007)
+    n_best = np.argmin(self.criterion_)
+
+    self.alpha_ = alphas_[n_best]
+    self.coef_ = coef_path_[:, n_best]
+    self._set_intercept(Xmean, ymean, Xstd)
+    return self
