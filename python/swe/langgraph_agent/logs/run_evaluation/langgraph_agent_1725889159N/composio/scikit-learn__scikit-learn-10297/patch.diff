diff --git a/sklearn/linear_model/ridge.py b/sklearn/linear_model/ridge.py
index 911c74a..8847520 100644
--- a/sklearn/linear_model/ridge.py
+++ b/sklearn/linear_model/ridge.py
@@ -1262,12 +1262,10 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
         the estimates. Larger values specify stronger regularization.
         Alpha corresponds to ``C^-1`` in other linear models such as
         LogisticRegression or LinearSVC.
-
-    fit_intercept : boolean
+    fit_intercept : boolean, default True
         Whether to calculate the intercept for this model. If set
         to false, no intercept will be used in calculations
-        (e.g. data is expected to be already centered).
-
+        (i.e. data is expected to be centered).
     normalize : boolean, optional, default False
         This parameter is ignored when ``fit_intercept`` is set to False.
         If True, the regressors X will be normalized before regression by
@@ -1275,24 +1273,20 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
         If you wish to standardize, please use
         :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``
         on an estimator with ``normalize=False``.
-
     scoring : string, callable or None, optional, default: None
         A string (see model evaluation documentation) or
         a scorer callable object / function with signature
         ``scorer(estimator, X, y)``.
-
+        If None, the negative mean squared error if the estimator is a
+        regressor and the accuracy score if the estimator is a classifier
+        are used.
     cv : int, cross-validation generator or an iterable, optional
         Determines the cross-validation splitting strategy.
         Possible inputs for cv are:
-
-        - None, to use the efficient Leave-One-Out cross-validation
-        - integer, to specify the number of folds.
-        - An object to be used as a cross-validation generator.
-        - An iterable yielding train/test splits.
-
-        Refer :ref:`User Guide <cross_validation>` for the various
-        cross-validation strategies that can be used here.
-
+          - None, to use the efficient Leave-One-Out cross-validation
+          - integer, to specify the number of folds.
+          - An object to be used as a cross-validation generator.
+          - An iterable yielding train/test splits.
     class_weight : dict or 'balanced', optional
         Weights associated with classes in the form ``{class_label: weight}``.
         If not given, all classes are supposed to have weight one.
@@ -1301,11 +1295,13 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
         weights inversely proportional to class frequencies in the input data
         as ``n_samples / (n_classes * np.bincount(y))``
 
-    Attributes
-    ----------
-    cv_values_ : array, shape = [n_samples, n_alphas] or \
-    shape = [n_samples, n_responses, n_alphas], optional
-        Cross-validation values for each alpha (if `store_cv_values=True` and
+        Note that these weights will be multiplied with sample_weight (passed
+        through the fit method) if sample_weight is specified.
+    store_cv_values : boolean, default=False
+        Flag indicating if the cross-validation values corresponding to
+        each alpha should be stored in the `cv_values_` attribute (see
+        below). This flag is only compatible with `cv=None` (i.e. using
+        Generalized Cross-Validation).
     `cv=None`). After `fit()` has been called, this attribute will contain \
     the mean squared errors (by default) or the values of the \
     `{loss,score}_func` function (if provided in the constructor).
@@ -1333,12 +1329,12 @@ class RidgeClassifierCV(LinearClassifierMixin, _BaseRidgeCV):
     advantage of the multi-variate response support in Ridge.
     """
     def __init__(self, alphas=(0.1, 1.0, 10.0), fit_intercept=True,
-                 normalize=False, scoring=None, cv=None, class_weight=None):
+                 normalize=False, scoring=None, cv=None, class_weight=None,
+                 store_cv_values=False):
         super(RidgeClassifierCV, self).__init__(
             alphas=alphas, fit_intercept=fit_intercept, normalize=normalize,
-            scoring=scoring, cv=cv)
+            scoring=scoring, cv=cv, store_cv_values=store_cv_values)
         self.class_weight = class_weight
-
     def fit(self, X, y, sample_weight=None):
         """Fit the ridge classifier.
 
diff --git a/sklearn/linear_model/tests/test_ridge.py b/sklearn/linear_model/tests/test_ridge.py
index ee44da5..2cae013 100644
--- a/sklearn/linear_model/tests/test_ridge.py
+++ b/sklearn/linear_model/tests/test_ridge.py
@@ -859,4 +859,29 @@ def test_dtype_match_cholesky():
     assert coef_64.dtype == X_64.dtype
     assert ridge_32.predict(X_32).dtype == X_32.dtype
     assert ridge_64.predict(X_64).dtype == X_64.dtype
-    assert_almost_equal(ridge_32.coef_, ridge_64.coef_, decimal=5)
+
+def test_ridge_classifier_cv_store_cv_values():
+    # Test RidgeClassifierCV with store_cv_values=True
+    rng = np.random.RandomState(42)
+    X = rng.rand(100, 10)
+    y = rng.randint(0, 2, 100)
+    
+    # Test with default cv=None (which should use efficient Leave-One-Out CV)
+    clf = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True, cv=None)
+    clf.fit(X, y)
+    
+    assert_true(hasattr(clf, 'cv_values_'))
+    assert_equal(clf.cv_values_.shape, (100, 3))  # 100 samples, 3 alphas
+    
+    # Test with explicit cv
+    clf = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0], store_cv_values=True, cv=5)
+    clf.fit(X, y)
+    
+    assert_true(hasattr(clf, 'cv_values_'))
+    assert_equal(clf.cv_values_.shape, (100, 3))  # 100 samples, 3 alphas
+    
+    # Test that cv_values_ is not stored when store_cv_values=False
+    clf = RidgeClassifierCV(alphas=[0.1, 1.0, 10.0], store_cv_values=False)
+    clf.fit(X, y)
+    
+    assert_true(not hasattr(clf, 'cv_values_'))
