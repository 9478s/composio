diff --git a/sklearn/linear_model/logistic.py b/sklearn/linear_model/logistic.py
index e4ea696..66a0e45 100644
--- a/sklearn/linear_model/logistic.py
+++ b/sklearn/linear_model/logistic.py
@@ -922,7 +922,17 @@ def _log_reg_scoring_path(X, y, train, test, pos_class=None, Cs=10,
         check_input=False, max_squared_sum=max_squared_sum,
         sample_weight=sample_weight)
 
-    log_reg = LogisticRegression(fit_intercept=fit_intercept)
+    log_reg = LogisticRegression(
+        fit_intercept=fit_intercept,
+        multi_class=multi_class,
+        solver=solver,
+        penalty=penalty,
+        dual=dual,
+        tol=tol,
+        max_iter=max_iter,
+        random_state=random_state,
+        C=1.0  # Using a default value since C is not directly available
+    )
 
     # The score method of Logistic Regression has a classes_ attribute.
     if multi_class == 'ovr':
diff --git a/sklearn/linear_model/tests/test_logistic.py b/sklearn/linear_model/tests/test_logistic.py
index 56be87f..0364a52 100644
--- a/sklearn/linear_model/tests/test_logistic.py
+++ b/sklearn/linear_model/tests/test_logistic.py
@@ -1260,3 +1260,56 @@ def test_warm_start_converge_LR():
         lr_ws.fit(X, y)
     lr_ws_loss = log_loss(y, lr_ws.predict_proba(X))
     assert_allclose(lr_no_ws_loss, lr_ws_loss, rtol=1e-5)
+
+def test_log_reg_scoring_path_multi_class():
+    from sklearn.linear_model.logistic import _log_reg_scoring_path
+    from sklearn.datasets import make_classification
+    from sklearn.model_selection import train_test_split
+    from sklearn.metrics import log_loss
+
+    # Generate a multi-class classification problem
+    X, y = make_classification(n_samples=100, n_features=20, n_classes=3, n_informative=10, random_state=42)
+
+    # Split the data
+    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
+
+    # Define parameters
+    Cs = [0.1, 1, 10]
+    scoring = 'neg_log_loss'
+
+    # Test for 'ovr' multi_class option
+    coefs_ovr, Cs_ovr, scores_ovr, _ = _log_reg_scoring_path(
+        X, y, train=range(len(y_train)), test=range(len(y_train), len(y)),
+        Cs=Cs, scoring=scoring, fit_intercept=True, multi_class='ovr'
+    )
+
+    # Test for 'multinomial' multi_class option
+    coefs_multi, Cs_multi, scores_multi, _ = _log_reg_scoring_path(
+        X, y, train=range(len(y_train)), test=range(len(y_train), len(y)),
+        Cs=Cs, scoring=scoring, fit_intercept=True, multi_class='multinomial'
+    )
+
+    # Check that the Cs are the same for both methods
+    assert_array_almost_equal(Cs_ovr, Cs_multi)
+
+    # Check that the scores are different for 'ovr' and 'multinomial'
+    assert_true(np.any(np.abs(scores_ovr - scores_multi) > 1e-3))
+
+    # Verify that the scores are consistent with log_loss
+    for i, C in enumerate(Cs):
+        lr_ovr = LogisticRegression(C=C, multi_class='ovr', fit_intercept=True)
+        lr_ovr.fit(X_train, y_train)
+        y_pred_ovr = lr_ovr.predict_proba(X_test)
+        score_ovr = -log_loss(y_test, y_pred_ovr)
+        assert_almost_equal(score_ovr, scores_ovr[i], decimal=3)
+
+        lr_multi = LogisticRegression(C=C, multi_class='multinomial', fit_intercept=True)
+        lr_multi.fit(X_train, y_train)
+        y_pred_multi = lr_multi.predict_proba(X_test)
+        score_multi = -log_loss(y_test, y_pred_multi)
+        assert_almost_equal(score_multi, scores_multi[i], decimal=3)
+
+    # Check that the coefficients have the correct shape
+    n_classes = len(np.unique(y))
+    assert_equal(coefs_ovr.shape, (len(Cs), n_classes, X.shape[1] + 1))
+    assert_equal(coefs_multi.shape, (len(Cs), n_classes, X.shape[1] + 1))
